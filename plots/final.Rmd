---
title: "House Pricing Prediction"
subtitle: "DS502 Final Project"
author: "Yufei Lin, Jingfeng Xia, Jinhong Yu, Shijing Yang, Yanze Wang"
date: "Nov 29 2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# check R version
R.Version()$major

# set up document
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
knitr::opts_chunk$set(fig.width=5,fig.height=3)
library(pander)
library(knitr)
library(skimr)
library(kableExtra)
library(tinytex)
library(dplyr)
library(purrr)
local({
  hook_inline = knitr::knit_hooks$get('inline')
  knitr::knit_hooks$set(inline = function(x) {
    res = hook_inline(x)
    if (is.numeric(x)) sprintf('$%s$', res) else res
  })
})

# define printf function
printf <- function(...)print(sprintf(...))
```

```{r libraries, include=FALSE}
# Import model libraries
library(pls)
library(randomForest)
library(gam)
library(glmnet)
library(ggplot2)
library(corrplot)
library(tidyverse)
library(caret)
library(mgcv)
library(Metrics)
library(visreg)
library(boot)
library('ggthemes') 
library('scales')
library('mice')
library('data.table')
library('gridExtra') 
library('GGally')
library('e1071')
library(Rmisc)
library(ggrepel)
library(psych)
library(xgboost)

```

# Introduction

## Description of the Problem

Being able to predict the price of a house tends to be an important skill for both the seller and consumers. For the seller, they could make better sales and consumers could have better understanding when they try to make a purchase. Therefore, in this project, we are planning to make prediction of house price based on the 79 different predictors provided by Kaggle dataset to determine values of residential homes in Ames, Iowa. We have noticed Sale Price has a typical right-skewed distribution, and decided to process it using two ways, logrithmic with base $e$ and square root for a distribution that is much closer to the shape of a Gaussian distribution for better performance in models like linear regression. In this analysis, we will perform random forest on original y-value for importance of variables and all the rest of models on both absolute and processed y-values to see which way would each model do better and provide an ensemble of models at the end of our study. 

## Description of the Dataset

In terms of the dataset, the entire data set consists of two pieces of data organized as training data set and test data set respectively. Whereas for each of the dataset, approximately 80 columns corresponding parameters would be evaluated with the prediction of house price. Some noteworthy predictors include the location classification, utilities, environment of neighborhood, house style and condition, area, year of built, and number of functioning rooms. There are over 1400 row of data points in both the training data set and the test data set. The sale prices in the train dataset are given as a parameter in the form of five or six figure full flat integers. The test data set will be applied to different regression models in order to distinguish the disparities of different model performances. 
    
## Approaches

Given that our data is aimed at predicting Sale Price of a house, it is unreasonable to require a model to fit the exact value of the dataset but only to reach an estimation within a certain range. Therefore, we have decided to use both regression and classification approaches to look at the problem on both the original and processed value. For regression method, we are going to look at if a prediction is within the range of the actual price $\pm 5\%$, we will say it is an accurate prediction. For classification prediction, we will be tagging the data into several different groups, and would be fitting the threshold accordingly with models like SVM and K-Means clustering. 

# Data Processing

## Read in Data

We have chosen to eliminate the Id column from this dataset because Id has nothing to do with our prediction and would mess up our prediction. We save data in "train.csv"" from Kaggle into a variable named \textbf{HousePricing} for further processing and we will separate it into training and testing set. For each model Bootstrapping will be performed before each model's training process.   

```{r readData}
HousePricing = read.csv("./SourceData/train_new.csv")
dim(HousePricing)
HousePricing = subset(HousePricing,select=-Id)

```

## Data Exploration

```{r dimensionOfData}
# The number of columns and rows
paste("Original training data set has",dim(HousePricing)[1], "rows and", dim(HousePricing)[2], "columns")

# # The percentage of data missing in train
# paste("The percentage of data missing in the original training data set is ", round(sum(is.na(HousePricing)) / (nrow(HousePricing) *ncol(HousePricing)),4)*100,"%",sep = "")

# The number of duplicated rows
paste("The number of duplicated rows are", nrow(HousePricing) - nrow(unique(HousePricing)))
```

```{r numOfNumericAndFactors}
paste("Number of Categorical Predictors:",sum(sapply(HousePricing[,1:84],typeof) == "character"))

paste("Number of Numeric Predictors:",dim(HousePricing)[2]-sum(sapply(HousePricing[,1:84],typeof) == "character")-1)

paste("Number of target label:", 1)

```


### Target Varaible vs. Predictors

```{r saleprice, fig.width=5,fig.height=3}
summary(HousePricing$SalePrice)
hist(HousePricing$SalePrice,col="blue",breaks = 25,main = "Distribution of Sale Price", xlab = "Sale Price")
```


\textbf{Conclusion}

It deviates from normal distribution and it is right skewed

```{r correlation, fig.width=4, fig.height=4}

numericVars <- which(sapply(HousePricing, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on
# cat('There are', length(numericVars), 'numeric variables')

all_numVar <- HousePricing[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs") #correlations of all numeric variables

#sort on decreasing correlations with SalePrice
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
 #select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")
```

\textbf{Conclusion}

The correlation graph above shows that the predictors that have 0.5 or greater correlation with our target variable SalePrice. By looking at the plot above, we can see that 2 out of 4 columns that are newly added seem to have descent correlation with our target variable. According to the plot, neighborhoods with higher median income residents tend to have higher price houses. This scenario totally makes sense since people with higher income tend to be able to afford a more expensive house in general. Also, crime index of a neighborhood seems to play an important role in deciding the house price of that area on average. The higher crime index an area has, the lower of the house price it tends to have. However, it also becomes clear that there are multicollinearity issues in our data set. For example, predictor GarageCars and GarageArea are strongly correlated (0.89) as well as predictor CrimeIndex and MedianIncome (-0.8), and they are all relatively strongly correlated to the SalePrice predictor.

### Overall Quality

```{r overall quality, fig.height=3, fig.width=5}
ggplot(data=HousePricing[!is.na(HousePricing$SalePrice),], aes(x=factor(OverallQual), y=SalePrice))+
        geom_boxplot(col='blue') + labs(x='Overall Quality') +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)
```
\textbf{Conclusion}
Overall Quality is the predictor that is the most strongly correlated with target SalesPrice. As we can see the graph above, it is clear a upward curve and proves its positive correlation with SalePrice.

### Plotting 'GrLivArea' too see if there are any outliers

```{R LivingAreaoutliers, fig.width=5,fig.height=3}
qplot(HousePricing$GrLivArea, HousePricing$SalePrice,col=HousePricing$GrLivArea>4500,xlab = "Ground Living Area (sqft)",ylab = "SalePrice")

pander(HousePricing[HousePricing$GrLivArea > 4500,][c("GrLivArea", "OverallQual","SalePrice")])

```

\textbf{Conclusion}

Predictor "Ground Living Area" is the second most important numeric feature for SalePrice. We plotted a scatter plot for this predictor versus sale price. As we can see from the plot, there are two points that appear to be outliers since the living area of both houses are big but having relatively low prices. However, sometimes houses with poor quality can also lead to low prices. In order to further explore these two houses, we also plotted the overall quality of these two houses. According to the table, they are both listed as 10 in terms of quality. So we can basically make an assumption that these two points are outliers and it is relatively safe to remove these points from the data set.

## Feature Engineering

In this section, we convert all missing value based on the following rules:

\begin{enumerate}
\item Categorical: fill in most common
\item Numeric: fill in median/average
\end{enumerate}

```{r featuerEngineering}
# remove ID column
HousePricing$Id = NULL
HousePricing = HousePricing[HousePricing$GrLivArea<4500,]

# for using later
numericVars <- which(sapply(HousePricing, is.numeric))
numericVarNames <- names(numericVars) 

# LotFrontage
# compute the median of neighbor, na.rm means compute medians without NA
paste("There are",dim(HousePricing[is.na(HousePricing$LotFrontage),])[1],"rows with NAs in LotFrontage column")
neighbor_Median  = HousePricing %>%
  select(LotFrontage, Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(LotFrontage = median(LotFrontage, na.rm = T))
print(neighbor_Median)

# replace the LotFrontage NA with its neighbor's Lotfrontage's median.
for (i in 1:nrow(HousePricing))
{
  if(is.na(HousePricing$LotFrontage[i])){
               HousePricing$LotFrontage[i] <- as.integer(median(HousePricing$LotFrontage[HousePricing$Neighborhood==HousePricing$Neighborhood[i]], na.rm=TRUE)) 
        }
}

# Alley, NA means no alley.
HousePricing$Alley[is.na(HousePricing$Alley)] = 'None'
HousePricing$Alley = as.factor(HousePricing$Alley)

# For utilites, there are two NAs, one row in one category, and the rest all share the same category
# Therefore we remove the entire column
# table(HousePricing$Utilities)
HousePricing$Utilities = NULL

# Pool variables are the ones with most NAs
# 1. Assign NAs to None (suppose those houses do not have a pool)
table(HousePricing$PoolQC)
HousePricing$PoolQC[is.na(HousePricing$PoolQC)] = "None"

# 2. Change it to Ordinal (scale them into numbers)
HousePricing$PoolQC=recode(HousePricing$PoolQC,'None' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)


# Fence
HousePricing$Fence[is.na(HousePricing$Fence)] = "None"
HousePricing$Fence = as.factor(HousePricing$Fence)

# Miscellaneous features
HousePricing$MiscFeature[is.na(HousePricing$MiscFeature)] = "None"
HousePricing$MiscFeature = as.factor(HousePricing$MiscFeature)



# garage
# replace NAs with the year that the house was built
HousePricing$GarageYrBlt[is.na(HousePricing$GarageYrBlt)] <- HousePricing$YearBuilt[is.na(HousePricing$GarageYrBlt)]
# garage type dost not seem to be ordinal, then convert to factors
HousePricing$GarageType[is.na(HousePricing$GarageType)] = "None"
HousePricing$GarageType = as.factor(HousePricing$GarageType)

# convert to ordinals
HousePricing$GarageFinish[is.na(HousePricing$GarageFinish)] = "None"
HousePricing$GarageFinish=recode(HousePricing$GarageFinish,'None' = 0,'Unf' = 1,'RFn' = 2,'Fin' = 3)
HousePricing$GarageQual[is.na(HousePricing$GarageQual)] = "None"
HousePricing$GarageQual=recode(HousePricing$GarageQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$GarageCond[is.na(HousePricing$GarageCond)] = "None"
HousePricing$GarageCond=recode(HousePricing$GarageCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$FireplaceQu[is.na(HousePricing$FireplaceQu)] = "None"
HousePricing$FireplaceQu=recode(HousePricing$FireplaceQu,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)

#electric
# only one missing value, convert it to most common type
HousePricing$Electrical[is.na(HousePricing$Electrical)] = "SBrkr"
HousePricing$Electrical = as.factor(HousePricing$Electrical)

# basement
length(which(is.na(HousePricing$BsmtQual) & is.na(HousePricing$BsmtCond) & is.na(HousePricing$BsmtExposure) & is.na(HousePricing$BsmtFinType1) & is.na(HousePricing$BsmtFinType2)))
HousePricing[!is.na(HousePricing$BsmtCond) & (is.na(HousePricing$BsmtFinType1)|is.na(HousePricing$BsmtQual)|is.na(HousePricing$BsmtExposure)|is.na(HousePricing$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
HousePricing$BsmtFinType2[333] = names(sort(table(HousePricing$BsmtFinType2),decreasing = TRUE))[1]
HousePricing$BsmtExposure[949] = names(sort(table(HousePricing$BsmtExposure),decreasing = TRUE))[1]

# convert to ordinal 
HousePricing$BsmtExposure[is.na(HousePricing$BsmtExposure)] = 'None'
HousePricing$BsmtExposure=recode(HousePricing$BsmtExposure,'None' = 0,'No' = 1,'Mn' = 2,'Av' = 3,'Gd' = 4)
HousePricing$BsmtQual[is.na(HousePricing$BsmtQual)] = 'None'
HousePricing$BsmtQual=recode(HousePricing$BsmtQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtCond[is.na(HousePricing$BsmtCond)] = 'None'
HousePricing$BsmtCond=recode(HousePricing$BsmtCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtFinType1[is.na(HousePricing$BsmtFinType1)] = 'None'
HousePricing$BsmtFinType1=recode(HousePricing$BsmtFinType1,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)
HousePricing$BsmtFinType2[is.na(HousePricing$BsmtFinType2)] = 'None'
HousePricing$BsmtFinType2=recode(HousePricing$BsmtFinType2,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)

# Mas
# missing value set to none
HousePricing$MasVnrType[is.na(HousePricing$MasVnrType)] = 'None'
HousePricing$MasVnrType = as.factor(HousePricing$MasVnrType)
HousePricing$MasVnrArea[(is.na(HousePricing$MasVnrArea))] = 0


# MS Zoning
# categorical --> factor
HousePricing$MSZoning = as.factor(HousePricing$MSZoning)

# street

# categorical --> factor
HousePricing$Street = as.factor(HousePricing$Street)
HousePricing$LotShape=recode(HousePricing$LotShape,'IR3' = 0,'IR2' = 1,'IR1' = 2,'Reg' =2)
HousePricing$LotConfig = as.factor(HousePricing$LotConfig)


# House condition
HousePricing$Condition1 = as.factor(HousePricing$Condition1)
HousePricing$Condition2 = as.factor(HousePricing$Condition2)

# categorical
HousePricing$LandContour = as.factor(HousePricing$LandContour)


# categorical
HousePricing$RoofStyle = as.factor(HousePricing$RoofStyle)

# ordinal
HousePricing$LandSlope=recode(HousePricing$LandSlope,'Sev' = 0,'Mod' = 1,'Gtl' = 2)


# categorical
HousePricing$BldgType = as.factor(HousePricing$BldgType)
HousePricing$HouseStyle=as.factor(HousePricing$HouseStyle)


HousePricing$RoofMatl=as.factor(HousePricing$RoofMatl)
HousePricing$Exterior1st=as.factor(HousePricing$Exterior1st)
HousePricing$Exterior2nd=as.factor(HousePricing$Exterior2nd)
HousePricing$ExterQual=recode(HousePricing$ExterQual,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$ExterCond=recode(HousePricing$ExterCond,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)

HousePricing$Foundation = as.factor(HousePricing$Foundation)
HousePricing$PavedDrive=recode(HousePricing$PavedDrive,'N' = 0,'P' = 1,'Y' = 2)
HousePricing$Heating = as.factor(HousePricing$Heating)
HousePricing$HeatingQC=recode(HousePricing$HeatingQC,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$CentralAir=recode(HousePricing$CentralAir,'N' = 0,'Y' = 1)

# Kitchen variables
HousePricing$KitchenQual=recode(HousePricing$KitchenQua,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)


HousePricing$Functional=recode(HousePricing$Functional,'Sal' = 0,'Sev' = 1,'Maj2' = 2,'Maj1' = 3,'Mod' = 4,'Min2' = 5,'Min1' = 6,'Typ' = 7)
# Neighborhood 
HousePricing$Neighborhood = as.factor(HousePricing$Neighborhood)
# Sale type
HousePricing$SaleType = as.factor(HousePricing$SaleType)
# Sale condition
HousePricing$SaleCondition = as.factor(HousePricing$SaleCondition)

# drop month sold
HousePricing$MoSold = NULL
HousePricing$MSSubClass = as.factor(HousePricing$MSSubClass)
# switch to factor 
HousePricing$MSSubClass=recode(HousePricing$MSSubClass,'20' = '1-STORY 1946+',
                               '30' = '1-STORY 1945-','40' = '1-STORY Unf Attic',
                               '45' = "1/2 STORY Unf Attic",'50' = '1/2 STORY Fin',
                               '60' = '2-STORY+','70' = '2-STORY 1945-','80' = 'SPLIT OR MULTI-LEVEL',
                               '85' = 'SPLIT FOYER','90' = 'DUPLEX', '120' = '1-STORY PUD 1946+',
                               '150' = '1/2 STORY PUD','160' = '2-STORY PUD 1946+',
                               '180' = 'PUD - MULTILEVEL',' 190' = '2 FAMILY CONVERSION')




```


Correlation between the numerical variables

```{r corNum}
# draw a plot of correlation between numerical variables in original data
#which(sapply(HousePricing, is.numeric))
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
cat('There are', length(numericVars), 'numeric variables, and', length(factorVars), 'categoric variables')
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
# CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
# cor_numVar <- cor_numVar[CorHigh, CorHigh]
# corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)

# draw a importance plot of all predictors in the original data
# set.seed(2018)
# quick_RF <- randomForest(x=HousePricing[,-78], y=HousePricing$SalePrice, ntree=100,importance=TRUE)
# imp_RF <- importance(quick_RF)
# imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
# imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
# ggplot(imp_DF[1:15,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")

```

```{r categorical predictors}
n1 <- ggplot(HousePricing[!is.na(HousePricing$SalePrice),], aes(x=Neighborhood, y=SalePrice)) +
        geom_bar(stat='summary', fun.y = "median", fill='blue') +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=50000), labels = comma) +
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3) +
        geom_hline(yintercept=163000, linetype="dashed", color = "red")
#dashed line is median SalePrice
n2 <- ggplot(data=HousePricing, aes(x=Neighborhood)) +
        geom_histogram(stat='count')+
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(n1, n2)

```

```{r ffe}
########################### further feature engineer ###################
# whether remod 
HousePricing$Remod = ifelse(HousePricing$YearBuilt == HousePricing$YearRemodAdd,0,1)
# the age of house 
HousePricing$Age = as.numeric(HousePricing$YrSold) - HousePricing$YearRemodAdd 
# whether is new
HousePricing$isnew = ifelse(HousePricing$YrSold == HousePricing$YearBuilt,1,0)
# total area.
HousePricing$TotalSqFeet = HousePricing$GrLivArea+HousePricing$TotalBsmtSF
# count the totol number of bathroom in the hourse
HousePricing$TotBathrooms <- HousePricing$FullBath + (HousePricing$HalfBath*0.5) + HousePricing$BsmtFullBath + (HousePricing$BsmtHalfBath*0.5)

HousePricing$TotalPorchSF <- HousePricing$OpenPorchSF + HousePricing$EnclosedPorch + HousePricing$X3SsnPorch + HousePricing$ScreenPorch

# draw a correlation plot after combine some variable
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)

# choose some true numerical variable to normalize(not include the encoded part)
numericVarNames <- numericVarNames[!(numericVarNames %in% c('MSSubClass', 'MoSold', 'YrSold', 'SalePrice', 'OverallQual', 'OverallCond'))]
numericVarNames <- append(numericVarNames, c('Age', 'TotalPorchSF', 'TotBathrooms', 'TotalSqFeet'))

# delete some highly correlated variables
HousePricing = subset(HousePricing, select = -c(GrLivArea,ExterQual,GarageArea,X1stFlrSF,
                                           TotRmsAbvGrd,TotalBsmtSF,GarageYrBlt,FullBath,
                                           HalfBath,YearRemodAdd,BsmtHalfBath,BsmtFullBath))
DFnumeric <- HousePricing[, names(HousePricing) %in% numericVarNames]
DFfactors <- HousePricing[, !(names(HousePricing) %in% numericVarNames)]
DFfactors <- DFfactors[, names(DFfactors) != 'SalePrice']

########################## Normalizing the numerical data #########################
predf = scale(DFnumeric,center = T,scale = T)

```

As discussed before, we have decided to use logrithmic with base $e$ and square root to process the data. We have also saved $15\%$ of our data into a variable named vault for the final test of each model. 

```{r getFinalData}
############ one-hot encoding and combine with scaled numerical data ###########
dfdummies = model.matrix(~.-1,DFfactors) %>% as.data.frame()
dim(dfdummies)
dim(predf)
newdata = cbind(predf,dfdummies)
newdata$SalePrice = HousePricing$SalePrice

set.seed(1)
vault = sample(1:nrow(newdata), nrow(newdata)*0.15)
dVault = newdata[vault,]
newdata = newdata[-vault,]
oriHouseP = newdata
oriHouseP$SalePrice = newdata$SalePrice
sumOri = summary(oriHouseP$SalePrice)
sumOri
logHouseP = newdata
logHouseP$SalePrice = log(newdata$SalePrice)
sumLog = summary(logHouseP$SalePrice)
hist(logHouseP$SalePrice, col="blue", main="Histogram of Logged Sale Price")
sumLog
sqrtHouseP = newdata
sqrtHouseP$SalePrice = '^'(newdata$SalePrice,1/4)
sumSqrt = summary(sqrtHouseP$SalePrice)
hist(sqrtHouseP$SalePrice, col="blue", main="Histogram of Forth Rooted Sale Price")
sumSqrt
```

```{r bootstrapingFunc, include=FALSE}
bsF<- function(datadf, randomizer){
  set.seed(randomizer)
  sample = sample(dim(datadf)[1],dim(datadf)[1],replace = T)
  btnewdata = datadf[sample,]
  return(btnewdata)
}
newOriHouseP = bsF(oriHouseP, 1234)
newLogHouseP = bsF(logHouseP, 1111)
newSqrtHouseP = bsF(sqrtHouseP, 1332)
```




## Seperate into Test and Training Set

Spearate by 70% train, 30% test. 

```{r saveToCSV}
toCsv <- function(df, fileName){
  set.seed(10)
  randS = sample(1:nrow(df), nrow(df)*0.7)
  train = df[randS,]
  test = df[-randS,]
  write.csv(train,paste("./SourceData/train_",fileName, ".csv",sep=""), row.names = FALSE)
  write.csv(test,paste("./SourceData/test_",fileName, ".csv",sep=""), row.names = FALSE)
}

toCsv(oriHouseP, "original")
toCsv(logHouseP, "log")
toCsv(sqrtHouseP, "sqrt")
```



```{r oriTestTrainReg, include=FALSE}
test_ori = read.csv("./SourceData/test_original.csv")
y_test_ori = test_ori$SalePrice
x_test_ori = subset (test_ori, select = -SalePrice)
train_ori = read.csv("./SourceData/train_original.csv")
y_train_ori = train_ori$SalePrice
x_train_ori = subset (train_ori, select = -SalePrice)
y_train_ori = as.numeric(y_train_ori)
y_test_ori = as.numeric(y_test_ori)
summary(y_train_ori)
```

```{r logTestTrainReg, include=FALSE}
test_log = read.csv("./SourceData/test_log.csv")
y_test_log = test_log$SalePrice
x_test_log = subset (test_log, select = -SalePrice)
train_log = read.csv("./SourceData/train_log.csv")
y_train_log = train_log$SalePrice
x_train_log = subset (train_log, select = -SalePrice)
y_train_log = as.numeric(y_train_log)
y_test_log = as.numeric(y_test_log)
summary(y_train_log)
```

```{r sqrtTestTrainReg, include=FALSE}
test_sqrt = read.csv("./SourceData/test_sqrt.csv")
y_test_sqrt = test_sqrt$SalePrice
x_test_sqrt = subset (test_sqrt, select = -SalePrice)
train_sqrt = read.csv("./SourceData/train_sqrt.csv")
y_train_sqrt = train_sqrt$SalePrice
x_train_sqrt = subset (train_sqrt, select = -SalePrice)
y_train_sqrt = as.numeric(y_train_sqrt)
y_test_sqrt = as.numeric(y_test_sqrt)
summary(y_train_sqrt)
```


# Prediction Algorithms

We choose to use PCR, Random Forest, GAM, Lasso and Ridge, Splines and Linear Regression to look at how each model would be suitable for our regression analysis. 

Each model needs a cross validation algorithm
Remember to report RMSE

## Regression Methods

### 1. Linear Regression 

#### Explanation

We have chosen this model to understand how each numeric variable is linear related to our House Price prediction.
```{r lr ori}
# ori
k = 5
lm_ori_accuracy = rep(0,k)
for (i in 1:k){
  set.seed(100+i)
  sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
  ori_train = train_ori[sample,]
  train = sample(nrow(ori_train),0.7*nrow(ori_train))
  training_dataset = ori_train[train,]
  validation_dataset = ori_train[-train,]
  lm_ori_model = glm(formula = SalePrice ~.,data = training_dataset)
  lm_ori_pred = predict(lm_ori_model,newdata = validation_dataset)
  lm_ori_accuracy[i] = mean(((validation_dataset$SalePrice - lm_ori_pred)/validation_dataset$SalePrice<=0.2) & ((validation_dataset$SalePrice - lm_ori_pred)/validation_dataset$SalePrice>=-0.05))
  
}
lm_ori_aver_accuracy = mean(lm_ori_accuracy)
```

```{r lr log}
k = 5
lm_log_accuracy = rep(0,k)
for (i in 1:k){
  set.seed(111+i)
  sample = sample(nrow(train_log),nrow(train_log),replace = T)
  log_train = train_log[sample,]
  train = sample(nrow(log_train),0.7*nrow(log_train))
  training_dataset = log_train[train,]
  validation_dataset = log_train[-train,]
  lm_log_model = glm(formula = SalePrice ~.,data = training_dataset)
  lm_log_pred = predict(lm_log_model,newdata = validation_dataset)
  lm_log_pred = exp(lm_log_pred)
  lm_log_accuracy[i] = mean(((exp(validation_dataset$SalePrice)-(lm_log_pred))/exp(validation_dataset$SalePrice)<=0.2) & ((exp(validation_dataset$SalePrice)-(lm_log_pred))/exp(validation_dataset$SalePrice)>=-0.05))
}
lm_log_aver_accuracy = mean(lm_log_accuracy)
```

```{r lr sqrt}
lm_sqrt_accuracy = rep(0,k)
for (i in 1:k){
  set.seed(150+i)
  sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
  sqrt_train = train_sqrt[sample,]
  train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
  training_dataset = sqrt_train[train,]
  validation_dataset = sqrt_train[-train,]
  lm_sqrt_model = glm(formula = SalePrice ~.,data = training_dataset)
  lm_sqrt_pred = predict(lm_sqrt_model,newdata = validation_dataset)
  lm_sqrt_pred = lm_sqrt_pred^4
  lm_sqrt_accuracy[i] = mean((((validation_dataset$SalePrice)^4- (lm_sqrt_pred))/(validation_dataset$SalePrice)^4<=0.2) & ((validation_dataset$SalePrice)^4- (lm_sqrt_pred))/(validation_dataset$SalePrice)^4>=-0.05)
}
lm_sqrt_aver_accuracy = mean(lm_sqrt_accuracy)
```

#### Check Accuracy

```{r lr2}
# Multiple R-squared: 0.9475, 
# Adjusted R-squared: 0.9345  
# F-statistic:  73.09  on 289 and 1170 DF, 
# p-value: < 2.2e-16

#accuracy for original 
lm_ori_accuracy
lm_ori_aver_accuracy
printf("We have the accuracy of the linear model approximately %.2f%%", lm_ori_aver_accuracy*100)

# accuracy for log
lm_log_accuracy
lm_log_aver_accuary = mean(lm_log_accuracy)
printf("We have the accuracy of the linear model after log transformation approximately %.2f%%", lm_log_aver_accuary*100)

## accuracy for sqrt
lm_sqrt_accuracy
lm_sqrt_aver_accuracy = mean(lm_sqrt_accuracy)
printf("We have the accuracy of the linear model after sqrt transformation approximately %.2f%%", lm_sqrt_aver_accuracy*100)

lm_accuracy_df = data.frame(lm = c('accuracy'),ori_accuracy = c(lm_ori_aver_accuracy),
                               log_accuracy = c(lm_log_aver_accuary),
                               sqrt_accuracy = c(lm_sqrt_aver_accuracy))
#lm_pred  = exp(lm_pred)
#result_lm_model = data.frame(Id = testing_data$Id, SalePrice = lm_pred)
```

### 2. Random Forest

#### Explanation

We have chosen this model because random forest is based on a collection of decision trees that could help us get better understanding of which tree and division contribute to which section such that we could have a better picture of the overall importance of each different factor in the prediction.

#### Prepare Model

We have $199$ independent variables in the data set, therefore we have set mtry(Number of randomly selected variables for each split) to be the square root of that number for maximum performance of the model. 

The following is the result from Random Forest algorithm: 

```{r rf-preparemodel_ori}
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_ori))
rfTrain_ori=randomForest(SalePrice~.,data=train_ori, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain_ori)
```

```{r rf-preparemodel_log}
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_log))
rfTrain_log=randomForest(SalePrice~.,data=train_log, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain_log)
```

```{r rf-preparemodel_sqrt}
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_sqrt))
rfTrain_sqrt=randomForest(SalePrice~.,data=train_sqrt, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain_sqrt)
```


#### Check Accuracy

We then need to check accuracy, as assumed before, we would look at whether the predicted data is within the given range. The following is the result. 

1) Accuracy for predicting original Data
```{r rfAccuracy_ori}
rfYhat_ori = predict(rfTrain_ori, newdata=x_test_ori)
#table(y_test, rfYhat)
rf_accuracy_ori = mean(((y_test_ori - rfYhat_ori)/y_test_ori<=0.2) & ((y_test_ori - rfYhat_ori)/y_test_ori>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", rf_accuracy_ori*100)
```

2) Accuracy for predicting logged data
```{r rfAccuracy_log}
rfYhat_log = predict(rfTrain_log, newdata=x_test_log)
#table(y_test, rfYhat)
rf_accuracy_log = mean(((exp(y_test_log) - exp(rfYhat_log))/exp(y_test_log)<=0.2) & ((exp(y_test_log) - exp(rfYhat_log))/exp(y_test_log)>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", rf_accuracy_log*100)
```

3) Accuracy for predicting data taken a power of $\frac{1}{4}$
```{r rfAccuracy_sqrt}
rfYhat_sqrt = predict(rfTrain_sqrt, newdata=x_test_sqrt)
#table(y_test, rfYhat)
rf_accuracy_sqrt = mean(((y_test_sqrt^4 - rfYhat_sqrt^4)/y_test_sqrt^4<=0.2) & ((y_test_sqrt^4 - rfYhat_sqrt^4)/y_test_sqrt^4>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", rf_accuracy_sqrt*100)
```

#### Variable Importance

Here we are going to show the top 10 most important variables in predicting sale price of a house.

```{r rfVarImportance1, fig.width=5,fig.height=3}
# another plot
importance = importance(rfTrain_ori)
importancedf =  data.frame(Variables = row.names(importance), MSE = importance[,1])
importancedf <- importancedf[order(importancedf$MSE, decreasing = TRUE),]
ggplot(importancedf[1:10,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= 'Importance') + coord_flip() + theme(legend.position="none")
```


From the random forest analysis, we have discovered that the top three most important factors for predicting sale price are the following:

\begin{enumerate}
\item OverallQual (Overall Quality of the building)
\item ExterQual (Evaluates the quality of the material on the exterior)
\item YearBuilt (The year the house is built)
\end{enumerate}

#### Cross Validation

In the cross validation, we have chosen to look at $R^2$, RMSE and MAE. 

```{r RFCV}
set.seed(123) 
  
# computing model performance metrics 
# pander(data.frame( R2 = R2(rfYhat, y_test_ori), 
#             RMSE = RMSE(rfYhat, y_test_ori), 
#             MAE = MAE(rfYhat, y_test_ori)), title="Cross Validation for Random Forest")
r2rf = c(R2(rfYhat_ori, y_test_ori),R2(exp(rfYhat_log), exp(y_test_log)),R2(rfYhat_sqrt^4, y_test_sqrt^4))
rmserf = c(RMSE(rfYhat_ori, y_test_ori),RMSE(exp(rfYhat_log), exp(y_test_log)),RMSE(rfYhat_sqrt^4, y_test_sqrt^4))
accrf = c(rf_accuracy_ori,rf_accuracy_log,rf_accuracy_sqrt)
```

### 3. PCR (Iris)

#### PCA

So far, our data totally has 216 columns of features (29 dense columns of numeric values and 187 sparse columns of one hot encoding values). In order to reduce the dimension of a mostly sparse feature matrix effectively, we applied the PCA method. Fortunately, we successfully reduced the dimension from 216 to no more than 32. 32 PCs have totally 99.997% of variance proportion. 29 PCs are enough for totally getting 85% of variance proportion. It is interesting that the number of PCs, 32, which is more than the number of dense columns, 29, shows that PCA has indeed extracted meaningful information from 187 sparse columns of one hot encoding and has successfully cut down the number of dimensions. 

```{r PCA} 

x_train_ori.pr = prcomp(x_train_ori) 

#print(summary(x_train_ori.pr)) 

``` 

In above table, we can find all variance proportions behind 32th PC are 0, which means 32 PCs are totally enough important to describe all features of the original data. In fact, we can take less than 32 PCs and 

find the best number of PCs with cross validation in training process. 

The sum of (0.1627, 0.1073, 0.105, 0.07973, 0.05255, 0.05155, 0.03586, 0.03352, 0.03141, 0.02978, 0.02679, 0.02574, 0.02376, 0.02327, 0.02185, 0.02102, 0.01885, 0.01826, 0.01709, 0.01591, 0.01533, 0.01396, 0.01287, 0.0110, 0.01046, 0.00921, 0.00687, 0.00639, 0.00596, 0.00374, 0.00187, 0.00037) is 0.99997, which means 32 PCs totally have 99.997% of variance proportion. Moreover, 29 PCs are enough for totally getting 85% of variance proportion. 

```{r pcaplot1} 

plot(c(1:32),x_train_ori.pr$sdev[1:32],xlab="Principal Components",type ="l",ylab="Prop. Variance Explained",main="Prop. Variance Elbow") 

``` 

This is generated by plotting all variance proportions of 32 PCs. We can see the elbow at 7 more clearly than the previous one. However, the square of tail on the right side of 7 is quite thick, which means maybe a number on the tail can be the best one for training. So, the best number of PC is between 7 and 32. Last but not the least, the exact best number can only be revealed by cross validation. 

  

```{r newdata} 

# New data list cleansed by PCA size of 868*32, which can take the place of original data.  

# Each column is one PC, PC's importance decreases by column number. 

# Try CV on number of PC you drop from right to left and know how many PCs are best to use. 

newdata = x_train_ori.pr$x[,1:32] 

printf("Rows num of training set: %i",nrow(x_train_ori)) 

printf("Columns num of training set: %i",ncol(x_train_ori)) 

printf("Rows num of training set after PCA: %i",nrow(newdata)) 

printf("Columns num of training set after PCA: %i",ncol(newdata)) 

``` 

This is to show the effect of dimension reduction and how can we take and use the new data modified by PCA method. 

### 3. PCR (Iris)

```{r PCA_description}
set.seed(2)
# PCA portion
train = sample(dim(train_ori)[1],size = dim(train_ori)[1]*0.7)
train.subset = train_ori[train,]
test = train_ori[-train,]
# bootstrap again
train.bs = sample(dim(train.subset)[1],replace = T)
train.df = train.subset[train.bs,]


# define a function to generate scree plots
pcaCharts <- function(x) {
  x.var <- x$sdev ^ 2
  x.pvar <- x.var/sum(x.var)
  # print("proportions of variance:")
  # print(x.pvar)
  
  par(mfrow=c(2,2))
  plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b',cex.lab=1, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
  plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b',cex.lab=0.9, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
  screeplot(x,cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.8)
  screeplot(x,type="l",cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.8)
  par(mfrow=c(1,1))
}

x = subset(train.df, select = -c(SalePrice) )
train.pca <- prcomp(x ,center = TRUE)
pcaCharts(train.pca)

########################### insert an image  (pca_plot.png) #################################
pcr_fit = pcr(SalePrice~., data = train.df,validation = "CV")
# summary(pcr_fit)
validationplot(pcr_fit, val.type = "MSEP",cex.lab=1, cex.axis=0.8, cex.main=1.5, cex.sub=0.8)
axis(side = 1, at = c(32), cex.axis=0.8)
abline(v = 32, col = "blue", lty = 5) 



########################### insert another image  (pcr_mse.png) #################################
```

\begin{figure}
\includegraphics{pcr_mse.png}
\end{figure}

\begin{centering}
Figure 3.1 .... 
\end{centering}

```{r graph1,echo=FALSE}
pcaCharts(train.pca)
```

\text The four graphs above show the variance explained for each component after doing PCA on our model. According to the graphs above, proportion of variance explained and variance are decreasing as number of components increases. Cumulative variance of the model increases as number of principal components increases. As we can see from the upper right graph, more than 90% of variance is explained when there are around 25 principal components. 

Then we further explore the details of variance explained for each number of components. We found out that when there are 32 components, the variance explained will reach 100% and the variance will reach 90% when there are 19 components. Since we wanted to obtain as much variance as we could, but also trying to limit the number of predictors, we calculated the cross validation MSE for each number components in a pcr model to get a relatively ideal number of components. As we can see from the graph below, the cross validation MSE is the lowest when there are 28 components in the PCR model. Therefore we applied 19 components to test the result of our testing data set. 

```{r graph2,echo=FALSE}
validationplot(pcr_fit, val.type = "MSEP",cex.lab=1, cex.axis=0.8, cex.main=1.5, cex.sub=0.8)
axis(side = 1, at = c(19), cex.axis=0.8)
abline(v = 19, col = "blue", lty = 5)
axis(side = 1, at = c(28), cex.axis=0.8)
abline(v = 28, col = "dark green", lty = 5)
```

```{r pcr_original,include=FALSE,echo=FALSE}
# start pcr

pcr_fit = pcr(SalePrice~., data = train.df,validation = "CV")
pcr_pred = predict(pcr_fit,test)

accuracy.ori = mean((test$SalePrice - pcr_pred[,,32])/test$SalePrice<=0.2 & (test$SalePrice - pcr_pred[,,32])/test$SalePrice>=-0.05)


rsq <- function(x, y) summary(lm(y~x))$r.squared
rsq.ori = rsq(test$SalePrice,pcr_pred[,,32])

rmse <-  function(m, o){sqrt(mean((m - o)^2))}
rmse.ori = rmse(test$SalePrice,pcr_pred[,,32])


```

``` {r pcr_log,include=FALSE}
train.log = read.csv("./SourceData/train_log.csv")
train_log = train.log
set.seed(667)
train = sample(dim(train_log)[1],size = dim(train_log)[1]*0.7)
train.subset = train_log[train,]
test = train_log[-train,]
# bootstrap again
train.bs = sample(dim(train.subset)[1],replace = T)
train.df = train.subset[train.bs,]
pcr_fit = pcr(SalePrice~., data = train.df,validation = "CV")
pcr_pred = predict(pcr_fit,test)
accuracy.log = mean((exp(test$SalePrice) - exp(pcr_pred[,,32]))/exp(test$SalePrice<=0.2) & (exp(test$SalePrice) - exp(pcr_pred[,,32]))/exp(test$SalePrice)>=-0.05)

rsq.log = rsq(test$SalePrice,pcr_pred[,,32])
rmse.log = rmse(exp(test$SalePrice),exp(pcr_pred[,,32]))

```

``` {r pcr_sqrt,include=FALSE}
train.sqrt = read.csv("./SourceData/train_sqrt.csv")
train_sqrt = train.sqrt

set.seed(233)
train = sample(dim(train_sqrt)[1],size = dim(train_sqrt)[1]*0.7)
train.subset = train_sqrt[train,]
test = train_sqrt[-train,]
# bootstrap again
train.bs = sample(dim(train.subset)[1],replace = T)
train.df = train.subset[train.bs,]
pcr_fit = pcr(SalePrice~., data = train.df,validation = "CV")
pcr_pred = predict(pcr_fit,test)
accuracy.sqrt = mean((test$SalePrice^4 - pcr_pred[,,32]^4)/test$SalePrice^4<=0.2 & (test$SalePrice^4 - pcr_pred[,,32]^4)/test$SalePrice^4>=-0.05)

rsq.sqrt = rsq(test$SalePrice,pcr_pred[,,32])
rmse.sqrt = rmse(test$SalePrice^4,pcr_pred[,,32]^4)
```

#### Cross Validatio

``` {r pcr_cv,include=FALSE }
x <- data.frame("y" = c("original","log","sqrt"), "Accuracy" = c(accuracy.ori,accuracy.log,accuracy.sqrt), "R2" = c(rsq.ori,rsq.log,rsq.sqrt),"RMSE" = c(rmse.ori,rmse.log,rmse.sqrt))
pander(x)

```

#### Cross Validation

### 4. Ridge Regression

#### Explanation

In our data, the number of the data is relatively small compared to the number of thr predictors, and we think there are some Multicollinearites within our predictors. All this would cause overfitting. In this circumstance, we think maybe linear regresion won??t perform pretty well. To prevent the overfitting and increase the explanatory power of the model, We will try to use some Shrinkage methods, in this case, we would use Ridge Regression and Lasso Regression. 

Ridge regression is very similar to linear regression, both try to minimize the RSS, but ridge regression has a penalty term-with L2 regularization, this could help us to prevent overfitting in our model. 

#### Prepare Model

1. First, we set initial alpha to 0 to fit the ridge regression,and set the values of initial lambda ranging from 10^10 to 10^(-2), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit.

```{r ridge }
# original
# original
Ridge_ori_accuracy= rep(0,k)
Ridge_ori_lambda = rep(0,k)
Ridge_ori_R2 = rep(0,k)
Ridge_ori_MAE = rep(0,k)
Ridge_ori_RMSE = rep(0,k)
Ridge_ori_coef = 0
Ridge_ori_train = rep(0,k)

for ( i in 1:k){
  set.seed(130+i)
  sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
  ori_train = train_ori[sample,]
  train = sample(nrow(ori_train),0.7*nrow(ori_train))
  training_dataset = ori_train[train,]
  validation_dataset = ori_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1300+i)
  grid=10^seq(10,-2, length =100)
  Ridge.ori.Alpha=0
  Ridge.ori.Fit = glmnet(X_train, y_train, alpha=Ridge.ori.Alpha, lambda=grid)
  Ridge.ori.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.ori.Alpha,nfolds = 10,type.measure = 'deviance')
  #Ridge.ori.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.ori.Alpha,lambda = grid )
  Ridge.ori.lambda = Ridge.ori.Fitcv$lambda.min
  Ridge_ori_lambda[i] = Ridge.ori.lambda
  Ridge.ori.Pred <- predict(Ridge.ori.Fit, s= Ridge.ori.lambda, newx = X_test)
  Ridge.ori.Pred1 = predict(Ridge.ori.Fit, s= Ridge.ori.lambda, newx = X_train)
  Ridge_ori_train[i] = mean(-0.05<=(((Ridge.ori.Pred1)-(y_train))/(y_train))&(((Ridge.ori.Pred1)-(y_train))/(y_train)) <=0.2)
  Ridge.ori.coef = predict(Ridge.ori.Fit, s = Ridge.ori.lambda, type="coefficients")
  Ridge_ori_accuracy[i] = mean(-0.05<=(((Ridge.ori.Pred)-(y_test))/(y_test))&(((Ridge.ori.Pred)-(y_test))/(y_test)) <=0.2)
  Ridge_ori_R2[i] = R2(Ridge.ori.Pred,(y_test))
  Ridge_ori_RMSE[i]= RMSE(Ridge.ori.Pred,(y_test))
  Ridge_ori_MAE[i] = MAE(Ridge.ori.Pred,(y_test))
}
Ridge_ori_aver_lambda = mean(Ridge_ori_lambda)
Ridge_ori_aver_accuracy = mean(Ridge_ori_accuracy)
Ridge_ori_aver_RMSE = mean(Ridge_ori_RMSE)
Ridge_ori_aver_R2 = mean(Ridge_ori_R2)
Ridge_ori_aver_MAE = mean(Ridge_ori_MAE)
Ridge_ori_aver_train = mean(Ridge_ori_train)

# log transformation
Ridge_log_accuracy= rep(0,k)
Ridge_log_lambda = rep(0,k)
Ridge_log_R2 = rep(0,k)
Ridge_log_MAE = rep(0,k)
Ridge_log_RMSE = rep(0,k)
Ridge_log_coef = 0
Ridge_log_train = rep(0,k)
for ( i in 1:k){
  set.seed(220+i)
  sample = sample(nrow(train_log),nrow(train_log),replace = T)
  log_train = train_log[sample,]
  train = sample(nrow(log_train),0.7*nrow(log_train))
  training_dataset = log_train[train,]
  validation_dataset = log_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1220+i)
  grid=10^seq(10,-2, length =100)
  Ridge.log.Alpha=0
  Ridge.log.Fit = glmnet(X_train, y_train, alpha=Ridge.log.Alpha, lambda=grid)
  Ridge.log.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.log.Alpha,nfolds = 10,type.measure = 'deviance')
  #Ridge.log.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.log.Alpha,lambda = grid )
  Ridge.log.lambda = Ridge.log.Fitcv$lambda.min
  Ridge_log_lambda[i] = Ridge.log.lambda
  Ridge.log.Pred1 = predict(Ridge.log.Fit, s= Ridge.log.lambda, newx = X_train)
  Ridge.log.Pred1 = exp(Ridge.log.Pred1)
  Ridge_log_train[i] = mean(-0.05<=(((Ridge.log.Pred1)-exp(y_train))/exp(y_train))&(((Ridge.log.Pred1)-exp(y_train))/exp(y_train)) <=0.2)
  Ridge.log.Pred = predict(Ridge.log.Fit, s= Ridge.log.lambda, newx = X_test)
  Ridge.log.Pred = exp(Ridge.log.Pred)
  Ridge.log.coef = predict(Ridge.log.Fit, s = Ridge.log.lambda, type="coefficients")
  Ridge_log_accuracy[i] = mean(-0.05<=(((Ridge.log.Pred)-exp(y_test))/exp(y_test))&(((Ridge.log.Pred)-exp(y_test))/exp(y_test)) <=0.2)
  Ridge_log_R2[i] = R2(Ridge.log.Pred,exp(y_test))
  Ridge_log_RMSE[i]= RMSE(Ridge.log.Pred,exp(y_test))
  Ridge_log_MAE[i] = MAE(Ridge.log.Pred,exp(y_test))
}
Ridge_log_aver_lambda = mean(Ridge_log_lambda)
Ridge_log_aver_accuracy = mean(Ridge_log_accuracy)
Ridge_log_aver_RMSE = mean(Ridge_log_RMSE)
Ridge_log_aver_R2 = mean(Ridge_log_R2)
Ridge_log_aver_MAE = mean(Ridge_log_MAE)
Ridge_log_aver_train = mean(Ridge_log_train)


# sqrt 
Ridge_sqrt_accuracy= rep(0,k)
Ridge_sqrt_lambda = rep(0,k)
Ridge_sqrt_R2 = rep(0,k)
Ridge_sqrt_MAE = rep(0,k)
Ridge_sqrt_RMSE = rep(0,k)
Ridge_sqrt_coef = 0
Ridge_sqrt_train = rep(0,k)
for ( i in 1:k){
  set.seed(240+i)
  sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
  sqrt_train = train_sqrt[sample,]
  train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
  training_dataset = sqrt_train[train,]
  validation_dataset = sqrt_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1240+i)
  grid=10^seq(10,-2, length =100)
  Ridge.sqrt.Alpha=0
  Ridge.sqrt.Fit = glmnet(X_train, y_train, alpha=Ridge.sqrt.Alpha, lambda=grid)
  Ridge.sqrt.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.sqrt.Alpha,nfolds = 10,type.measure = 'deviance')
  Ridge.sqrt.lambda = Ridge.sqrt.Fitcv$lambda.min
  Ridge_sqrt_lambda[i] = Ridge.sqrt.lambda
  Ridge.sqrt.Pred1 = predict(Ridge.sqrt.Fit, s= Ridge.sqrt.lambda, newx = X_train)
  Ridge.sqrt.Pred1 = (Ridge.sqrt.Pred1)^4
  Ridge_sqrt_train[i] = mean(-0.05<=(((Ridge.sqrt.Pred1)-(y_train)^4)/(y_train)^4)&(((Ridge.sqrt.Pred1)-(y_train)^4)/(y_train)^4) <=0.2)
  Ridge.sqrt.Pred = predict(Ridge.sqrt.Fit, s= Ridge.sqrt.lambda, newx = X_test)
  Ridge.sqrt.Pred = (Ridge.sqrt.Pred)^4
  Ridge.sqrt.coef = predict(Ridge.sqrt.Fit, s = Ridge.sqrt.lambda, type="coefficients")
  Ridge_sqrt_accuracy[i] = mean(-0.05<=(((Ridge.sqrt.Pred)-(y_test)^4)/(y_test)^4)&(((Ridge.sqrt.Pred)-(y_test)^4)/(y_test)^4) <=0.2)
  Ridge_sqrt_R2[i] = R2(Ridge.sqrt.Pred,(y_test)^4)
  Ridge_sqrt_RMSE[i]= RMSE(Ridge.sqrt.Pred,(y_test)^4)
  Ridge_sqrt_MAE[i] = MAE(Ridge.sqrt.Pred,(y_test)^4)
}
Ridge_sqrt_aver_lambda = mean(Ridge_sqrt_lambda)
Ridge_sqrt_aver_accuracy = mean(Ridge_sqrt_accuracy)
Ridge_sqrt_aver_RMSE = mean(Ridge_sqrt_RMSE)
Ridge_sqrt_aver_R2 = mean(Ridge_sqrt_R2)
Ridge_sqrt_aver_MAE = mean(Ridge_sqrt_MAE)
Ridge_sqrt_aver_train = mean(Ridge_sqrt_train)
```

This is the optimal lambda computed by cross validation during the model training for Ridge Regression. as the following: 

```{r ridgeOplam}
# lambda of original 
Ridge_ori_aver_lambda
# lambda of log transformation 
Ridge_log_aver_lambda
# lambda of sqrt transformation
Ridge_sqrt_aver_lambda

#par(mfrow=c(2,2))
# plot(Ridge_ori_Fitcv)
# plot(Ridge_log_Fitcv)
# plot(Ridge_sqrt_Fitcv)
#(mfrow = c(1,1))
```

#### Check Accuracy 

We then need to check accuracy, as assumed before, we would look at whether the difference between predicted SalePrice and true SalePrice is within the range we define as accurate prediction. And compare the three reuslts computed by different type of SalePrice. The following is the result. 

```{r ridgeAcc}
# original accuracy
printf("Accuracy of Ridge is approximately %.2f%%", Ridge_ori_aver_accuracy*100)

# log accuracy
printf("Accuracy of Ridge with Log Transformation is approximately %.2f%%", Ridge_log_aver_accuracy*100)

# sqrt accuracy
printf("Accuracy of Ridge with Sqrt Transformation is approximately %.2f%%", Ridge_sqrt_aver_accuracy*100)

# accuracy dataframe
Ridge_accuracy_df = data.frame(Ridge = c('training accuracy','testing accuracy'),
                               ori_accuracy =c(Ridge_ori_aver_train,Ridge_ori_aver_accuracy),
                               log_accuracy =c(Ridge_log_aver_train,Ridge_log_aver_accuracy),
                            sqrt_accuracy=c(Ridge_sqrt_aver_train,Ridge_sqrt_aver_accuracy))

```

#### Cross Validation 

Then let us take a look at the MSE and R^2 of this model, and compare with the result got from different SalePrice transformation. 

```{r ridgeCV}
# ori
Ridge_ori_RMSE
pander(data.frame(R2 = Ridge_ori_aver_R2,RMSE = Ridge_ori_aver_RMSE,MAE = Ridge_ori_aver_MAE),title="Cross Validation of Ridge Regression")
# log
Ridge_log_RMSE
pander(data.frame(R2 = Ridge_log_aver_R2, RMSE = Ridge_log_aver_RMSE, MAE = Ridge_log_aver_MAE ), title="Cross Validation of Ridge Regression After Log Transformation")
# sqrt
Ridge_sqrt_RMSE
pander(data.frame(R2 = Ridge_sqrt_aver_R2, RMSE = Ridge_sqrt_aver_RMSE, MAE = Ridge_sqrt_aver_MAE), title="Cross Validation of Ridge Regression After Sqrt Transformation")
```

### 5. Lasso Regression

#### Explanation 

Lasso is very similar to Ridge regression in that a penalty term is added to the regression optimization function(OLS) to prevent overfitting and  reduce the effect of Multicollinearity. 

But compare to ridge, Lasso use the L1-regulartion, which will shrink some parameters to 0, this could use for parameter selection and make the result more interpretable, could help us find out which parameter is what Lasso thinks is important.  

#### Prepare Model 

1. Set the initial alpha is equal to 1 (Ridge regression is 0), and also use the same initial lambda, then try to use cross validation to choose the optimal lambda for Lasso after using different type of SalePrice 

According to different type of SalePrice, we conducted the model training for five times to each, and see how the model performened on average and whether the transformation in SalePrice will make the prediction better. 

```{r lasso}
# ori
k=5
Lasso_ori_accuracy= rep(0,k)
Lasso_ori_lambda = rep(0,k)
Lasso_ori_R2 = rep(0,k)
Lasso_ori_MAE = rep(0,k)
Lasso_ori_RMSE = rep(0,k)
Lasso_ori_coef = 0
Lasso_ori_train = rep(0,k)
for ( i in 1:k){
  set.seed(330+i)
  sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
  ori_train = train_ori[sample,]
  train = sample(nrow(ori_train),0.7*nrow(ori_train))
  training_dataset = ori_train[train,]
  validation_dataset = ori_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1330+i)
  grid=10^seq(10,-2, length =100)
  Lasso.ori.Alpha=1
  Lasso.ori.Fit = glmnet(X_train, y_train, alpha=Lasso.ori.Alpha, lambda=grid)
  Lasso.ori.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.ori.Alpha,nfolds = 10,type.measure = 'deviance')
  Lasso.ori.lambda = Lasso.ori.Fitcv$lambda.min
  Lasso_ori_lambda[i] = Lasso.ori.lambda
  Lasso.ori.Pred <- predict(Lasso.ori.Fit, s= Lasso.ori.lambda, newx = X_test)
  Lasso.ori.Pred1 <- predict(Lasso.ori.Fit, s= Lasso.ori.lambda, newx = X_train)
  Lasso.ori.coef = predict(Lasso.ori.Fit, s = Lasso.ori.lambda, type="coefficients")
  Lasso_ori_coef = Lasso_ori_coef + Lasso.ori.coef 
  Lasso_ori_train[i] = mean(-0.05<=(((Lasso.ori.Pred1)-(y_train))/(y_train))&(((Lasso.ori.Pred1)-(y_train))/(y_train)) <=0.2)
  Lasso_ori_accuracy[i] = mean(-0.05<=(((Lasso.ori.Pred)-(y_test))/(y_test))&(((Lasso.ori.Pred)-(y_test))/(y_test)) <=0.2)
  Lasso_ori_R2[i] = R2(Lasso.ori.Pred,(y_test))
  Lasso_ori_RMSE[i]= RMSE(Lasso.ori.Pred,(y_test))
  Lasso_ori_MAE[i] = MAE(Lasso.ori.Pred,(y_test))
}  
Lasso_ori_aver_lambda = mean(Lasso_ori_lambda)
Lasso_ori_aver_accuracy = mean(Lasso_ori_accuracy)
Lasso_ori_aver_RMSE = mean(Lasso_ori_RMSE)
Lasso_ori_aver_R2 = mean(Lasso_ori_R2)
Lasso_ori_aver_MAE = mean(Lasso_ori_MAE)
Lasso_ori_aver_train = mean(Lasso_ori_train)
n = length(Lasso.ori.coef)
Lasso_ori_aver_coef = (Lasso_ori_coef/5)[1:n,]


k = 5
Lasso_log_accuracy= rep(0,k)
Lasso_log_lambda = rep(0,k)
Lasso_log_R2 = rep(0,k)
Lasso_log_MAE = rep(0,k)
Lasso_log_RMSE = rep(0,k)
Lasso_log_coef = 0
Lasso_log_train = rep(0,k)
for ( i in 1:k){
  set.seed(60+i)
  sample = sample(nrow(train_log),nrow(train_log),replace = T)
  log_train = train_log[sample,]
  train = sample(nrow(log_train),0.7*nrow(log_train))
  training_dataset = log_train[train,]
  validation_dataset = log_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(150+i)
  grid=10^seq(10,-2, length =100)
  Lasso.log.Alpha=1
  Lasso.log.Fit = glmnet(X_train, y_train, alpha=Lasso.log.Alpha, lambda=grid)
  #Lasso.log.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.log.Alpha,nfolds = 10,type.measure = 'deviance')
  Lasso.log.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.log.Alpha,lambda = grid)
  Lasso.log.lambda = Lasso.log.Fitcv$lambda.min
  Lasso_log_lambda[i] = Lasso.log.lambda
  Lasso.log.Pred <- predict(Lasso.log.Fit, s= Lasso.log.lambda, newx = X_test)
  Lasso.log.Pred = exp(Lasso.log.Pred)
  Lasso.log.Pred1 <- predict(Lasso.log.Fit, s= Lasso.log.lambda, newx = X_train)
  Lasso.log.Pred1 = exp(Lasso.log.Pred1)
  Lasso.log.coef = predict(Lasso.log.Fit, s = Lasso.log.lambda, type="coefficients")
  Lasso_log_coef = Lasso_log_coef + Lasso.log.coef
  Lasso_log_train[i] = mean(-0.05<=(((Lasso.log.Pred1)-exp(y_train))/exp(y_train))&(((Lasso.log.Pred1)-exp(y_train))/exp(y_train)) <=0.2)
  Lasso_log_accuracy[i] = mean(-0.05<=(((Lasso.log.Pred)-exp(y_test))/exp(y_test))&(((Lasso.log.Pred)-exp(y_test))/exp(y_test)) <=0.2)
  Lasso_log_R2[i] = R2(Lasso.log.Pred,exp(y_test))
  Lasso_log_RMSE[i]= RMSE(Lasso.log.Pred,exp(y_test))
  Lasso_log_MAE[i] = MAE(Lasso.log.Pred,exp(y_test))
}  
n = length(Lasso_log_coef)
Lasso_log_aver_coef = (Lasso_log_coef/5)[1:n,]
Lasso_log_aver_lambda = mean(Lasso_log_lambda)
Lasso_log_aver_accuracy = mean(Lasso_log_accuracy)
Lasso_log_aver_RMSE = mean(Lasso_log_RMSE)
Lasso_log_aver_R2 = mean(Lasso_log_R2)
Lasso_log_aver_MAE = mean(Lasso_log_MAE)
Lasso_log_aver_train = mean(Lasso_log_train)



# sqrt transformation
#sqrt
Lasso_sqrt_accuracy= rep(0,k)
Lasso_sqrt_lambda = rep(0,k)
Lasso_sqrt_R2 = rep(0,k)
Lasso_sqrt_MAE = rep(0,k)
Lasso_sqrt_RMSE = rep(0,k)
Lasso_sqrt_coef = 0
Lasso_sqrt_train = rep(0,k)
for ( i in 1:k){
  set.seed(100+i)
  sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
  sqrt_train = train_sqrt[sample,]
  train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
  training_dataset = sqrt_train[train,]
  validation_dataset = sqrt_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1370+i)
  grid=10^seq(10,-2, length =100)
  Lasso.sqrt.Alpha=1
  Lasso.sqrt.Fit = glmnet(X_train, y_train, alpha=Lasso.sqrt.Alpha, lambda=grid)
  Lasso.sqrt.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.sqrt.Alpha,nfolds = 10,type.measure = 'deviance')
  Lasso.sqrt.lambda = Lasso.sqrt.Fitcv$lambda.min
  Lasso_sqrt_lambda[i] = Lasso.sqrt.lambda
  Lasso.sqrt.Pred = predict(Lasso.sqrt.Fit, s= Lasso.sqrt.lambda, newx = X_test)
  Lasso.sqrt.Pred = Lasso.sqrt.Pred^4
  Lasso.sqrt.Pred1 = predict(Lasso.sqrt.Fit, s= Lasso.sqrt.lambda, newx = X_train)
  Lasso.sqrt.Pred1 = Lasso.sqrt.Pred1^4
  Lasso.sqrt.coef = predict(Lasso.sqrt.Fit, s = Lasso.sqrt.lambda, type="coefficients")
  Lasso_sqrt_coef = Lasso_sqrt_coef + Lasso.sqrt.coef 
  Lasso_sqrt_train[i] = mean(-0.05<=(((Lasso.sqrt.Pred1)-(y_train)^4)/(y_train)^4)&(((Lasso.sqrt.Pred1)-(y_train)^4)/(y_train)^4) <=0.2)
  Lasso_sqrt_accuracy[i] = mean(-0.05<=(((Lasso.sqrt.Pred)-(y_test)^4)/(y_test)^4)&(((Lasso.sqrt.Pred)-(y_test)^4)/(y_test)^4) <=0.2)
  Lasso_sqrt_R2[i] = R2(Lasso.sqrt.Pred,(y_test)^4)
  Lasso_sqrt_RMSE[i]= RMSE(Lasso.sqrt.Pred,(y_test)^4)
  Lasso_sqrt_MAE[i] = MAE(Lasso.sqrt.Pred,(y_test)^4)
}  
n = length(Lasso_sqrt_coef)
Lasso_sqrt_aver_coef = (Lasso_sqrt_coef/5)[1:n,]
Lasso_sqrt_aver_lambda = mean(Lasso_sqrt_lambda)
Lasso_sqrt_aver_accuracy = mean(Lasso_sqrt_accuracy)
Lasso_sqrt_aver_RMSE = mean(Lasso_sqrt_RMSE)
Lasso_sqrt_aver_R2 = mean(Lasso_sqrt_R2)
Lasso_sqrt_aver_MAE = mean(Lasso_sqrt_MAE)
Lasso_sqrt_aver_train = mean(Lasso_sqrt_train)

```

This is the optimal lambda computed by cross validation during the model training for Lasso Regression. as the following: 

```{r lassoOplam}
# lambda of original 
Lasso_ori_aver_lambda
# lambda of log transformation 
Lasso_log_aver_lambda
# lambda of sqrt transformation
Lasso_sqrt_aver_lambda

#par(mfrow=c(2,2))
# plot(Lasso.ori.Fitcv)
# plot(Lasso.log.Fitcv)
# plot(Lasso.sqrt.Fitcv)
#par(mfrow = c(1,1))
```

#### Coefficient From Lasso Regression 

Here we are going to show the predictors lasso choosed. 

```{r lassoImp}
# coefficient chose by Lasso
names(head(sort(Lasso_ori_aver_coef[Lasso_ori_aver_coef!=0][-1],decreasing = TRUE),10))

# coefficient chose by Lasso after log transformation 
names(head(sort(Lasso_log_aver_coef[Lasso_log_aver_coef!=0][-1],decreasing = TRUE),10))

# coefficient chose by Lassoafter sqrt transformation 
names(head(sort(Lasso_sqrt_aver_coef[Lasso_sqrt_aver_coef!=0][-1],decreasing = TRUE),10))
```

#### Check accuracy 

We then need to check accuracy, as assumed before, we would look at whether the difference between predicted SalePrice and true SalePrice is within the range we define as accurate prediction. And compare the three reuslts computed by different type of SalePrice, to see whether this kind of transforamtion will make the prediction more accurate. The following is the result. 

```{r lassoAcc}
# original accuracy
printf("Accuracy of Lasso is approximately %.2f%%", Lasso_ori_aver_accuracy*100)
# log accuracy
printf("Accuracy of Lasso with Log Transformation is approximately %.2f%%", Lasso_log_aver_accuracy*100)
# sqrt accuracy
printf("Accuracy of Lasso with Sqrt Transformation is approximately %.2f%%", Lasso_sqrt_aver_accuracy*100)

# accuracy dataframe
Lasso_accuracy_df = data.frame(Lasso = c('training accuracy','testing accuracy'),
                               ori_accuracy =c(Lasso_ori_aver_train,Lasso_ori_aver_accuracy),
                               log_accuracy =c(Lasso_log_aver_train,Lasso_log_aver_accuracy),
                            sqrt_accuracy=c(Lasso_sqrt_aver_train,Lasso_sqrt_aver_accuracy))
```

#### Cross Validation 

Then let us take a look at the MSE and R^2 of this model, and compare with the result got from different SalePrice transformation. 

```{r lassoCV}
# ori
Lasso_ori_RMSE
Lasso_ori_aver_RMSE = mean(Lasso_ori_RMSE)
Lasso_ori_aver_R2 = mean(Lasso_ori_R2)
Lasso_ori_aver_MAE = mean(Lasso_ori_MAE)
pander(data.frame(R2 = Lasso_ori_aver_R2,RMSE = Lasso_ori_aver_RMSE,MAE = Lasso_ori_aver_MAE),title="Cross Validation of Lasso Regression")
# log
Lasso_log_RMSE
Lasso_log_aver_RMSE = mean(Lasso_log_RMSE)
Lasso_log_aver_R2 = mean(Lasso_log_R2)
Lasso_log_aver_MAE = mean(Lasso_log_MAE)
pander(data.frame(R2 = Lasso_log_aver_R2, RMSE = Lasso_log_aver_RMSE, MAE = Lasso_log_aver_MAE ), title="Cross Validation of Lasso Regression After Log Transformation")
# sqrt
Lasso_sqrt_RMSE
Lasso_sqrt_aver_RMSE = mean(Lasso_sqrt_RMSE)
Lasso_sqrt_aver_R2 = mean(Lasso_sqrt_R2)
Lasso_sqrt_aver_MAE = mean(Lasso_sqrt_MAE)
pander(data.frame(R2 = Lasso_sqrt_aver_R2, RMSE = Lasso_sqrt_aver_RMSE, MAE = Lasso_sqrt_aver_MAE), title="Cross Validation of Lasso Regression After Sqrt Transformation")

```

### 6. GAM

#### Explanation

We have chosen GAM as one of our models because it produces an analysis on those factors that have less linear relationship with the result, for instance LotFrontage, YearRemodAdd, and MasVnrArea that are having relatively high importance but also high p-value that makes them not very linear related to SalePrice. 

1) GAM1

In this model, we have LotFrontage, YearRemodAdd and MasVnrArea as predictors, with YearRemodAdd having a degree of freedom 2. We obtain the following result:

```{r GAM1, fig.width=5,fig.height=6}
# fit1 = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train_ori)
# printf("Deviance of Model 1 approximately %.2f", deviance(fit1))
# pred1 = predict(fit1, newdata=x_test_ori)
# par(mfrow=c(2,1))
# accuracy = mean(abs(y_test_ori - pred1)/y_test_ori<=0.05)
# printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
# plot(fit1 , se=TRUE , col="red")
# 
# fit1_log = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train_log)
# printf("Deviance of Model 1 approximately %.2f", deviance(fit1_log))
# pred1 = predict(fit1, newdata=x_test_log)
# par(mfrow=c(2,1))
# accuracy = mean(abs(y_test_log - pred1)/y_test_log<=0.05)
# printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
# plot(fit1_log , se=TRUE , col="red")
# 
# fit1_sqrt = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train_sqrt)
# printf("Deviance of Model 1 approximately %.2f", deviance(fit1_sqrt))
# pred1 = predict(fit1, newdata=x_test_sqrt)
# par(mfrow=c(2,1))
# accuracy = mean(abs(y_test_sqrt - pred1)/y_test_sqrt<=0.05)
# printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
# plot(fit1_sqrt , se=TRUE , col="red")
```

2) GAM2

In this model, we have LotFrontage, YearRemodAdd and MasVnrArea as predictors. None of them has a degree of freedom in the fit. We obtain the following result:

```{r GAM2, fig.width=5,fig.height=3}
# fit2 = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train_ori)
# printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
# pred2 = predict(fit2, newdata=x_test_ori)
# accuracy = mean(abs(y_test_ori - pred2)/y_test_ori<=0.05)
# printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
# plot(fit2 , se=TRUE , col="red")
# 
# fit2_log = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train_log)
# printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
# pred2 = predict(fit2, newdata=x_test_log)
# accuracy = mean(abs(y_test_log - pred2)/y_test_log<=0.05)
# printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
# plot(fit2_log , se=TRUE , col="red")
# 
# fit2_sqrt = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train_sqrt)
# printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
# pred2 = predict(fit2, newdata=x_test_sqrt)
# accuracy = mean(abs(y_test_sqrt - pred2)/y_test_sqrt<=0.05)
# printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
# plot(fit2_sqrt , se=TRUE , col="red")
```

3) GAM3

In this model, we have LotFrontage, YearRemodAdd and MasVnrArea as predictorswith LotFrontage having a degree of freedom of 3. We obtain the following result:

```{r GAM3, fig.width=5,fig.height=3}
# fit3 = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train_ori)
# printf("Deviance of Model 3 approximately %.2f", deviance(fit3))
# pred3 = predict(fit3, newdata=x_test_ori)
# accuracy = mean(abs(y_test_ori - pred3)/y_test_ori<=0.05)
# printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
# plot(fit3 , se=TRUE , col="red")
# 
# fit3_log = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train_log)
# printf("Deviance of Model 3 approximately %.2f", deviance(fit3_log))
# pred3 = predict(fit3_log, newdata=x_test_log)
# accuracy = mean(abs(y_test_log - pred3)/y_test_log<=0.05)
# printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
# plot(fit3_log, se=TRUE , col="red")
# 
# fit3_sqrt = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train_sqrt)
# printf("Deviance of Model 3 approximately %.2f", deviance(fit3))
# pred3 = predict(fit3_sqrt, newdata=x_test_sqrt)
# accuracy = mean(abs(y_test_sqrt - pred3)/y_test_sqrt<=0.05)
# printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
# plot(fit3_sqrt , se=TRUE , col="red")
```

GAM Summary

We then take an ANOVA test to understand which model is the best and we have the following result:

```{r GAMSummary}
# anova(fit1,fit2,fit3,test="F")
# anova(fit1_log,fit2_log,fit3_log,test="F")
# anova(fit1_sqrt,fit2_sqrt,fit3_sqrt,test="F")
```
We can see that from the anova test that P-value for the second model is the smallest, therefore, it is the most preferred.  

#### Cross Validation

Then, we conduct a cross-validation on the second model only. 

```{r GAMCV}
# set.seed(123) 
# 
# pander(data.frame( R2 = R2(pred2, y_test_ori), 
#             RMSE = RMSE(pred2, y_test_ori), 
#             MAE = MAE(pred2, y_test_ori)), title="Cross Validation of Model 2")
```





# Evaluation of different models

Root MSE

# Choose best fit model

# Conclusion
1. Classfication 

# Discussion & Future Development

# Resources






















































